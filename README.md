# Deep Residual Learning for Image Recognition ğŸ”¬

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.9+-orange.svg)](https://pytorch.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![CIFAR-10](https://img.shields.io/badge/Dataset-CIFAR--10-red.svg)](https://www.cs.toronto.edu/~kriz/cifar.html)

> **Systematic implementation and analysis of ResNet architecture on CIFAR-10, demonstrating how residual connections solve the degradation problem in deep neural networks.**

## ğŸš€ Quick Results Overview

### Performance Comparison
![Test Accuracy Comparison 20-layer](results/test_accuracy_comparison_20.png)
*20-layer networks: ResNet-20 achieves 91.95% vs Plain-20's 90.84%*

![Test Accuracy Comparison 56-layer](results/test_accuracy_comparison_56.png) 
*56-layer networks: ResNet-56 achieves 93.25% vs Plain-56's 86.71% - demonstrating the degradation problem*

## ğŸ“Š Key Results Summary

| Architecture | Layers | Parameters | Test Accuracy | Improvement | Degradation Effect |
|--------------|--------|------------|---------------|-------------|-------------------|
| Plain-20     | 20     | 0.27M      | 90.84%        | -           | -                 |
| **ResNet-20** | 20     | 0.27M      | **91.95%**    | +1.11%      | -                 |
| Plain-56     | 56     | 0.85M      | 86.71%        | -           | **-4.13%** ğŸ“‰    |
| **ResNet-56** | 56     | 0.85M      | **93.25%**    | +6.54%      | **+1.30%** ğŸ“ˆ    |

**Key Finding**: ResNet benefits amplify 6Ã— as network depth increases (1.11% â†’ 6.54% improvement)

## ğŸ¯ Problem Statement

Deep neural networks suffer from the **degradation problem**: as networks get deeper, training accuracy saturates and then degrades rapidly. This is not due to overfitting but optimization difficulties. Our implementation demonstrates how **residual connections** solve this fundamental issue.

## ğŸ“ˆ Training Dynamics Analysis

### 20-Layer Networks
![Training Dynamics 20-layer](results/accuracy_loss_20.png)
*Training curves showing ResNet-20's superior convergence speed and stability*

### 56-Layer Networks  
![Training Dynamics 56-layer](results/accuracy_loss_56.png)
*Dramatic difference in 56-layer networks: Plain-56 struggles while ResNet-56 converges smoothly*

**Observations:**
- ResNets converge **2Ã— faster** than plain networks
- Plain networks show training instability that worsens with depth
- Residual connections provide consistent gradient flow

## ğŸ” Classification Quality Analysis

### 20-Layer Confusion Matrices
![Confusion Matrix 20-layer](results/confusion_20.png)
*Classification performance comparison: ResNet-20 shows stronger diagonal patterns*

### 56-Layer Confusion Matrices
![Confusion Matrix 56-layer](results/confusion_56.png)
*ResNet-56 demonstrates superior classification across all CIFAR-10 classes with sharper predictions*

## âš¡ Quick Start

```bash
# Clone the repository
git clone https://github.com/vinayak1729-web/Deep_Residual_Learning_VS_Deep_learning_for_ImageRecognition.git
cd Deep_Residual_Learning_VS_Deep_learning_for_ImageRecognition

# Install dependencies
pip install torch torchvision matplotlib numpy seaborn

# Run 20-layer comparison
jupyter notebook notebooks/20_layers_Deep_Residual_Learning_VS_Deep_learning_for__ImageRecognition.ipynb

# Run 56-layer comparison  
jupyter notebook notebooks/56_layers_deep-residual-learning-vs-deep-learning-for-image_classification_.ipynb
```

## ğŸ—ï¸ Architecture Overview

### Network Design
- **Formula**: 6n + 2 layers (n=3 for 20-layer, n=9 for 56-layer)
- **Stages**: 3 stages with filter progression [16, 32, 64]
- **Blocks**: Each block contains 2Ã—(3Ã—3 conv + BatchNorm + ReLU)

### Residual Connection Concept
```python
# Plain Network
output = F(x)

# Residual Network  
output = F(x) + x  # Identity shortcut
```

### Custom ResNet Block Implementation
```python
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        residual = self.shortcut(x)
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += residual
        return F.relu(out)
```

## ğŸ”§ Training Configuration

### Hyperparameters
- **Optimizer**: SGD (momentum=0.9, weight_decay=1e-4)
- **Learning Rate**: 0.1 â†’ 0.01 (epoch 80) â†’ 0.001 (epoch 120)
- **Batch Size**: 128
- **Epochs**: 200
- **Data Augmentation**: 4-pixel padding + random crop + horizontal flip

### Dataset
- **CIFAR-10**: 32Ã—32 RGB images, 10 classes
- **Training samples**: 50,000
- **Test samples**: 10,000
- **Normalization**: Standard CIFAR-10 statistics

## ğŸ§ª Key Findings & Theoretical Insights

### 1. **Degradation Problem Validation** âœ…
Plain-56 (86.71%) performs **worse** than Plain-20 (90.84%), confirming optimization difficulties in deep plain networks.

### 2. **Residual Benefits Amplify with Depth** ğŸ“ˆ
- 20-layer improvement: +1.11%
- 56-layer improvement: +6.54% (**6Ã— amplification**)

### 3. **Identity Mapping Hypothesis** ğŸ¯
Skip connections enable layers to learn identity mappings when beneficial, preventing performance degradation.

### 4. **Gradient Highway Effect** ğŸ›¤ï¸
Residual connections create direct paths for gradients, solving vanishing gradient problems in deep networks.


## ğŸš€ Performance Optimization & Recommendations

### Computational Efficiency
- **20-layer**: ~0.27M parameters, suitable for resource-constrained environments
- **56-layer**: ~0.85M parameters, optimal accuracy-efficiency trade-off

### Best Practices
- âœ… Use ResNets for networks **>20-30 layers**
- âœ… The computational overhead is justified by significant accuracy gains  
- âœ… Particularly beneficial for high-accuracy applications
- âœ… Consider deeper networks when model performance plateaus

## ğŸ“ Educational Value & Applications

This implementation serves as:
- **ğŸ“š Reference Implementation**: Faithful reproduction of ResNet principles
- **ğŸ“Š Comparative Study**: Systematic analysis of plain vs residual networks
- **ğŸ”¬ Empirical Validation**: Concrete evidence for theoretical concepts
- **ğŸ› ï¸ Practical Guide**: Real-world deep network design insights

### Learning Outcomes
Students and practitioners will understand:
- Why deep networks are hard to train
- How residual connections solve optimization problems
- The relationship between network depth and performance
- Practical implementation of modern CNN architectures

## ğŸ“ˆ Future Extensions

### Potential Improvements
- **Wide ResNets**: Explore width vs depth trade-offs
- **ResNeXt**: Implement grouped convolutions
- **DenseNet**: Compare with dense connections
- **Other Datasets**: Extend to ImageNet, CIFAR-100
- **Attention Mechanisms**: Integrate modern attention modules

## ğŸ“– Citations

### This Work
```bibtex
@article{shinde2025resnet,
  title={Deep Residual Learning for Image Recognition: Implementation and Analysis},
  author={Shinde, Vinayak Raju},
  year={2025},
  month={September}
}
```

### Original ResNet Paper
```bibtex
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
```

## ğŸ¤ Contributing

Contributions are welcome! Areas for contribution:
- ğŸ› **Bug Reports**: Found an issue? Let us know!
- ğŸ’¡ **Feature Requests**: Suggest new experiments or analyses
- ğŸ”€ **Pull Requests**: Improve code, documentation, or add features
- ğŸ“– **Documentation**: Help make the project more accessible
- ğŸ§ª **Experiments**: Try different architectures or datasets

### Contribution Guidelines
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“¬ Contact & Support

**Vinayak Raju Shinde**
- ğŸ“§ **Email**: [shindevinayak233@gmail.com](mailto:shindevinayak233@gmail.com)
- ğŸ”— **LinkedIn**: [Vinayak Shinde](https://www.linkedin.com/in/vinayak-shinde-1aa968223/)
- ğŸ™ **GitHub**: [@vinayak1729-web](https://github.com/vinayak1729-web)

### Get Help
- ğŸ’¬ **Issues**: Open a GitHub issue for bug reports or questions
- ğŸ“– **Discussions**: Use GitHub Discussions for broader conversations
- ğŸ“ **Academic Collaboration**: Reach out for research collaborations

---

<div align="center">

### ğŸŒŸ **Impact & Recognition**

**â­ Star this repository if it helped you understand ResNets better!**

*This implementation has helped researchers and students understand the fundamental principles of deep residual learning through hands-on experimentation and comprehensive analysis.*

**ğŸ“Š Research Stats**: 6 comprehensive experiments | 93.25% peak accuracy | 6Ã— amplification effect demonstrated

Made with â¤ï¸ for the deep learning community

</div>
